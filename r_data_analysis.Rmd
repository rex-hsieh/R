---
title: "R: Data Analysis Workflow"
author: Meng Hsuan Hsieh
output:
  html_notebook:
    theme: united
    toc: yes
---

We will now get into data science using R. This assumes a lot of maturity in the language; please consult the previous notebooks if you're unsure about some of the techniques presented here.

# Data Types

One of the packages in the base R distribution is called datasets, and it is entirely filled with example datasets. While you’ll be lucky if any of them are suited to your particular area of research, they are ideal for testing your code and for exploring new techniques.

```{r}
data()
data("kidney", package = "survival")
kidney
```

Let's be real: we cannot expect to use these datasets other than testing purposes. As such, the more interesting question is **how can we import datasets and work with them from there**. There are many, many formats and standards of text documents for storing data. Com‐ mon formats for storing data are delimiter-separated values (CSV or tab-delimited), eXtensible Markup Language (XML), JavaScript Object Notation (JSON), and YAML (which recursively stands for YAML Ain’t Markup Language). Other sources of text data are less well-structured—a book, for example, contains text data without any formal (that is, standardized and machine parsable) structure. 

The main advantage of storing data in text files is that they can be read by more or less all other data analysis software and by humans. This makes your data more widely reusable by others.

## CSV

Rectangular (spreadsheet-like) data is commonly stored in delimited-value files, par‐ ticularly comma-separated values (CSV) and tab-delimited values files. The read.table function reads these delimited files and stores the results in a data frame. In its simplest form, it just takes the path to a text file and imports the contents.

Example:

```{r}
library(learningr)
deer_file <- system.file(
  "extdata",
  "RedDeerEndocranialVolume.dlm",
  package = "learningr"
  )
deer_data <- read.table(deer_file, header = TRUE, fill = TRUE)
str(deer_data, vec.len = 1) #vec.len alters the amount of output
head(deer_data)
```

Notice that the class of each column has been automatically determined, and row and column names have been automatically assigned. The column names are (by default) forced to be valid variable names (via make.names), and if row names aren’t provided the rows are simply numbered 1, 2, 3, and so on.

There are lots of arguments to specify how the file will be read; perhaps the most important is sep, which determines the character to use as a separator between fields. You can also specify how many lines of data to read via nrow, and how many lines at the start of the file to skip. More advanced options include the ability to override the default row names, column names, and classes, and to specify the character encoding of the input file and how string input columns should be declared.

Example on why it is important what is read:

```{r}
crab_file <- system.file(
  "extdata",
  "crabtag.csv",
  package = "learningr"
  )
(crab_id_block <- read.csv(
  crab_file, header = FALSE, skip = 3,
  nrow = 2
  )
)
```

And now we can build a proper dataset:

```{r}
(crab_tag_notebook <- read.csv(
  crab_file,
  header = FALSE,
  skip = 8,
  nrow = 5
  )
)
```

```{r}
(crab_lifetime_notebook <- read.csv(
  crab_file,
  header = FALSE,
  skip = 15,
  nrow = 3 
  )
)
```

For really low-level control when importing this sort of file, you can use the scan function, on which read.table is based. Ordinarily, you should never have to resort to scan, but it can be useful for malformed or nonstandard files.

The opposite task, writing files, is generally simpler than reading files, since you don’t need to worry about oddities in the file—you usually want to create something standard. read.table and read.csv have the obviously named counterparts write.table and write.csv.

Finally, functions take a data frame and a file path to write to:

```{r}
write.csv(
  crab_lifetime_notebook, "Data/Cleaned/crab lifetime data.csv", row.names = FALSE,
  fileEncoding = "utf8"
)
```


```{r}
library(xlsx)
bike_file <- system.file(
  "extdata",
  "Alpe d'Huez.xls",
  package = "learningr"
  )

bike_data <- read.xlsx2(
  bike_file,
  sheetIndex = 1,
  startRow =2,
  endRow = 38,
  colIndex = 2:8,
  colClasses = c(
    "character", "numeric", "character", "integer",
    "character", "character", "character"
    )
)

head(bike_data)
```

However, we will work mostly with datasets that can be imported easily with functions such as read.csv().


# Data Cleaning

After one imports the dataset, one needs to be extra careful about what is feasible in a dataset and what is not. It's almost always the wrong one for what you want to do with it, and no matter who gave it to you, it is almost always dirty. Cleaning and transforming data may not be the fun part of data analysis, but you will probably spend more of your life than you care to doing it. Fortunately, R has a wide selection of tools to help with these tasks.

## Cleaning Strings/Texts

Something that is well-cherished by the community of data cleaners is the ability to automate some of these onerous tasks. One of these tools is writing functions! If we are interested in cleaning strings, we can write a function that automatically does it for us.

A quick note: this is most useful when we have a clear idea on how the data looks like. If we know the data is not very complex in terms of mistakes, ie. there are not that many *types* of mistakes (this is NEVER the case, by the way), then a couple of functions would suffice in transforming the existing data to the desirable data types. A function that one can write is something like the following:

```{r}
yn_to_logical <- function(x){
  y <- rep.int(NA, length(x))
  y[x == "Y"] <- TRUE
  y[x == "N"] <- FALSE
  y
}
```
```{r}
library("learningr")
alpe_d_huez <- learningr::alpe_d_huez
alpe_d_huez
alpe_d_huez$DrugUse <- yn_to_logical(alpe_d_huez$DrugUse)
alpe_d_huez
```

This direct replacement of one string with another doesn't scale very well to having lots of choices of string. If you have ten thousand possible inputs, then a function to change each one would be very hard to write without errors, and even harder to maintain.

Fortunately, much more sophisticated manipulation is possible, and it is relatively easy to detect, extract, and replace parts of strings that match a pattern. R has a suite of builtin functions for handling these tasks, (loosely) based upon the Unix grep tool. They accept a string to manipulate and a regular expression to match. As mentioned in Chapter 1, regular expressions are patterns that provide a flexible means of describing the contents of a string. They are very useful for matching complex string-data types like phone numbers or email addresses.


The grep, grepl, and regexpr functions all find strings that match a pattern, and sub and gsub replace matching strings. In classic R style, these functions are meticulously correct and very powerful, but suffer from funny naming conventions, quirky argument ordering, and odd return values that have arisen for historical reasons.

Fortunately, in the same way that plyr provides a consistent wrapper around apply functions and lubridate provides a consistent wrapper around the date-time functions, the stringr package provides a consistent wrapper around the string manipulation functions. The difference is that while you will occasionally need to use a base apply or date-time function, stringr is advanced enough that you shouldn't need to bother with grep at all. So, take a look at the ?grep help page, but don't devote too much of your brain to it.

Now, the next example:

```{r}
data(english_monarchs, package = "learningr")
head(english_monarchs)
```

One of the problems with history is that there is an awful lot of it (PREACH, am I rite?). Fortunately, odd or messy data can be a really good indicator of the interesting bits of history, so we can narrow it down to the good stuff. For example, although there were seven territories that came together to form England, their boundaries were far from fixed, and sometimes one kingdom would conquer another. We can find these convergences by searching for commas in the domain column. To detect a pattern, we use the str_detect function. The fixed function tells str_detect that we are looking for a fixed string (a comma) rather than a regular expression. str_detect returns a logical vector that we can use for an index:

```{r}
library(stringr)
english_monarchs$domain

multiple_kingdoms <- str_detect(english_monarchs$domain, fixed(","))
# The str_detect function lets us detect the patterns in a particular list of dataset. Data cleaning is only possible when we know what is in our dataset.
english_monarchs[multiple_kingdoms, c("name", "domain")]
```

This time, since we are looking for two things, it is easier to specify a regular expression rather than a fixed string. The pipe character, |, has the same meaning in regular expressions as it does in R: it means "or".

In this next example, to prevent excessive output we just return the name column and ignore missing values (with is.na):

```{r}
multiple_rulers <- str_detect(english_monarchs$name, ",|and")
english_monarchs$name[multiple_rulers & !is.na(multiple_rulers)]
```

If we wanted to split the name column into individual rulers, then we could use str_split (or strsplit from base R, which does the same thing) in much the same way. str_split accepts a vector and returns a list, since each input string can be split into a vector of possibly differing lengths.

```{r}
individual_rulers <- str_split(english_monarchs$name, ", | and ")
head(individual_rulers[sapply(individual_rulers, length) > 1])
```

Now, we can count the strings types in each list of strings by writing functions to return counts to categories.

```{r}
english_monarchs$name
ap <- c("Ed", "Ec", "Ri") #can also use laply from plyr
sapply(
  ap,
  function(ap)
    {
    sum(str_count(english_monarchs$name, ap))
    }
)
```

It looks like the standard modern Latin spelling is most common in this dataset. If we want to replace the eths and thorns, we can use str_replace_all (a variant function, str_replace, replaces only the first match).

```{r}
fruits <- c("one apple", "two pears", "three bananas")
str_replace(fruits, "[aeiou]", "-")
```


```{r}
library(stringr)
english_monarchs$new_name <- str_replace_all(english_monarchs$name, "[ðþ]", "th")

gender <- c( "MALE", "Male", "male", "M", "FEMALE", "Female", "female", "f", NA )

clean_gender <- str_replace(gender, regex("^f(emale)?$",ignore_case = TRUE), "Female")
clean_gender

clean_gender <- str_replace(gender, regex("^m(ale)?$",ignore_case = TRUE), "Male")
clean_gender

str_replace("Toyota subaru on tarmac road with my TA", regex("ta",ignore_case = FALSE), "tA")
str_replace_all("Toyota subaru on tarmac road with my TA", regex("ta",ignore_case = FALSE), "tA")
```

More information on string replacement can be found [here](https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf).


## Editing Columns

Suppose we want to add a column to the english_monarchs data frame denoting the number of years the rulers were in power. We can use standard assignment to achieve this:

```{r}
english_monarchs$length.of.reign.years <-
  english_monarchs$end.of.reign - english_monarchs$start.of.reign
```

This works, but the repetition of the data frame variable names makes this a lot of effort to type and to read. The with function makes things easier by letting you call variables directly. It takes a data frame OR environment and an expression to evaluate:

```{r}
english_monarchs$length.of.reign.years <- with(
  english_monarchs,
  end.of.reign - start.of.reign
)
```

This is less clunky to write, and gives us exactly what we wanted!

The within() function works similarly, but returns the entire dataframe!

```{r}
english_monarchs <- within(
  english_monarchs,
  {
    length.of.reign.years <- end.of.reign - start.of.reign
  }
)
```

This becomes less clunky if we have many columns to edit and replace:

```{r}
english_monarchs <- within(
  english_monarchs,
  {
    length.of.reign.years <- end.of.reign - start.of.reign
    reign.was.more.than.30.years <- length.of.reign.years > 30
  }
)
```

A good heuristic is that if you are creating or changing one column, then use with; if you want to manipulate several columns at once, then use within.

An alternative approach is taken by the mutate function in the plyr package, which accepts new and revised columns as name-value pairs:

```{r}
# install.packages("plyr")
library("plyr")
english_monarchs <- mutate(
  english_monarchs,
  length.of.reign.years        = end.of.reign - start.of.reign,
  reign.was.more.than.30.years = length.of.reign.years > 30
)
```

The red deer dataset that we saw in the previous chapter contains measurements of the endocranial volume for each deer using four different techniques. For some but not all of the deer, a second measurement was taken to test the repeatability of the technique. This means that some of the rows have missing values. The complete.cases function tells us which rows are free of missing values:

```{r}
data("deer_endocranial_volume", package = "learningr")
has_all_measurements <- complete.cases(deer_endocranial_volume)
deer_endocranial_volume[has_all_measurements, ]
```

The na.omit function provides a shortcut to this, removing any rows of a data frame where there are missing values; by contrast, na.fail function spits out "failure" whenever the dataframe has a missing value:


```{r}
na.omit(deer_endocranial_volume)
na.fail(deer_endocranial_volume)
```

Both these functions can accept vectors as well, removing missing values or failing, as in the data frame case.

You can use multiple imputation to fill in missing values in a statistically sound way. This is beyond the scope of the book, but the mice and mix packages are good places to start.


## Converting Wide and Long Forms

The red deer dataset contains measurements of the volume of deer skulls obtained in four different ways. Each measurement for a particular deer is given in its own column.


