---
title: "R: Data Analysis Workflow"
author: Meng Hsuan Hsieh
output:
  html_notebook:
    theme: united
    toc: yes
---

We will now get into data science using R. This assumes a lot of maturity in the language; please consult the previous notebooks if you're unsure about some of the techniques presented here.

# Data Types

One of the packages in the base R distribution is called datasets, and it is entirely filled with example datasets. While you’ll be lucky if any of them are suited to your particular area of research, they are ideal for testing your code and for exploring new techniques.

```{r}
data()
data("kidney", package = "survival")
kidney
```

Let's be real: we cannot expect to use these datasets other than testing purposes. As such, the more interesting question is **how can we import datasets and work with them from there**. There are many, many formats and standards of text documents for storing data. Com‐ mon formats for storing data are delimiter-separated values (CSV or tab-delimited), eXtensible Markup Language (XML), JavaScript Object Notation (JSON), and YAML (which recursively stands for YAML Ain’t Markup Language). Other sources of text data are less well-structured—a book, for example, contains text data without any formal (that is, standardized and machine parsable) structure. 

The main advantage of storing data in text files is that they can be read by more or less all other data analysis software and by humans. This makes your data more widely reusable by others.

## CSV

Rectangular (spreadsheet-like) data is commonly stored in delimited-value files, par‐ ticularly comma-separated values (CSV) and tab-delimited values files. The read.table function reads these delimited files and stores the results in a data frame. In its simplest form, it just takes the path to a text file and imports the contents.

Example:

```{r}
library(learningr)
deer_file <- system.file(
  "extdata",
  "RedDeerEndocranialVolume.dlm",
  package = "learningr"
  )
deer_data <- read.table(deer_file, header = TRUE, fill = TRUE)
str(deer_data, vec.len = 1) #vec.len alters the amount of output
head(deer_data)
```

Notice that the class of each column has been automatically determined, and row and column names have been automatically assigned. The column names are (by default) forced to be valid variable names (via make.names), and if row names aren’t provided the rows are simply numbered 1, 2, 3, and so on.

There are lots of arguments to specify how the file will be read; perhaps the most important is sep, which determines the character to use as a separator between fields. You can also specify how many lines of data to read via nrow, and how many lines at the start of the file to skip. More advanced options include the ability to override the default row names, column names, and classes, and to specify the character encoding of the input file and how string input columns should be declared.

Example on why it is important what is read:

```{r}
crab_file <- system.file(
  "extdata",
  "crabtag.csv",
  package = "learningr"
  )
(crab_id_block <- read.csv(
  crab_file, header = FALSE, skip = 3,
  nrow = 2
  )
)
```

And now we can build a proper dataset:

```{r}
(crab_tag_notebook <- read.csv(
  crab_file,
  header = FALSE,
  skip = 8,
  nrow = 5
  )
)
```

```{r}
(crab_lifetime_notebook <- read.csv(
  crab_file,
  header = FALSE,
  skip = 15,
  nrow = 3 
  )
)
```

For really low-level control when importing this sort of file, you can use the scan function, on which read.table is based. Ordinarily, you should never have to resort to scan, but it can be useful for malformed or nonstandard files.

The opposite task, writing files, is generally simpler than reading files, since you don’t need to worry about oddities in the file—you usually want to create something standard. read.table and read.csv have the obviously named counterparts write.table and write.csv.

Finally, functions take a data frame and a file path to write to:

```{r}
write.csv(
  crab_lifetime_notebook, "Data/Cleaned/crab lifetime data.csv", row.names = FALSE,
  fileEncoding = "utf8"
)
```


```{r}
library(xlsx)
bike_file <- system.file(
  "extdata",
  "Alpe d'Huez.xls",
  package = "learningr"
  )

bike_data <- read.xlsx2(
  bike_file,
  sheetIndex = 1,
  startRow =2,
  endRow = 38,
  colIndex = 2:8,
  colClasses = c(
    "character", "numeric", "character", "integer",
    "character", "character", "character"
    )
)

head(bike_data)
```

However, we will work mostly with datasets that can be imported easily with functions such as read.csv().


# Data Cleaning

After one imports the dataset, one needs to be extra careful about what is feasible in a dataset and what is not. It's almost always the wrong one for what you want to do with it, and no matter who gave it to you, it is almost always dirty. Cleaning and transforming data may not be the fun part of data analysis, but you will probably spend more of your life than you care to doing it. Fortunately, R has a wide selection of tools to help with these tasks.

## Cleaning Strings/Texts

Something that is well-cherished by the community of data cleaners is the ability to automate some of these onerous tasks. One of these tools is writing functions! If we are interested in cleaning strings, we can write a function that automatically does it for us.

A quick note: this is most useful when we have a clear idea on how the data looks like. If we know the data is not very complex in terms of mistakes, ie. there are not that many *types* of mistakes (this is NEVER the case, by the way), then a couple of functions would suffice in transforming the existing data to the desirable data types. A function that one can write is something like the following:

```{r}
yn_to_logical <- function(x){
  y <- rep.int(NA, length(x))
  y[x == "Y"] <- TRUE
  y[x == "N"] <- FALSE
  y
}
```
```{r}
library("learningr")
alpe_d_huez <- learningr::alpe_d_huez
alpe_d_huez
alpe_d_huez$DrugUse <- yn_to_logical(alpe_d_huez$DrugUse)
alpe_d_huez
```

This direct replacement of one string with another doesn't scale very well to having lots of choices of string. If you have ten thousand possible inputs, then a function to change each one would be very hard to write without errors, and even harder to maintain.

Fortunately, much more sophisticated manipulation is possible, and it is relatively easy to detect, extract, and replace parts of strings that match a pattern. R has a suite of builtin functions for handling these tasks, (loosely) based upon the Unix grep tool. They accept a string to manipulate and a regular expression to match. As mentioned in Chapter 1, regular expressions are patterns that provide a flexible means of describing the contents of a string. They are very useful for matching complex string-data types like phone numbers or email addresses.


The grep, grepl, and regexpr functions all find strings that match a pattern, and sub and gsub replace matching strings. In classic R style, these functions are meticulously correct and very powerful, but suffer from funny naming conventions, quirky argument ordering, and odd return values that have arisen for historical reasons.

Fortunately, in the same way that plyr provides a consistent wrapper around apply functions and lubridate provides a consistent wrapper around the date-time functions, the stringr package provides a consistent wrapper around the string manipulation functions. The difference is that while you will occasionally need to use a base apply or date-time function, stringr is advanced enough that you shouldn't need to bother with grep at all. So, take a look at the ?grep help page, but don't devote too much of your brain to it.

Now, the next example:

```{r}
data(english_monarchs, package = "learningr")
head(english_monarchs)
```

One of the problems with history is that there is an awful lot of it (PREACH, am I rite?). Fortunately, odd or messy data can be a really good indicator of the interesting bits of history, so we can narrow it down to the good stuff. For example, although there were seven territories that came together to form England, their boundaries were far from fixed, and sometimes one kingdom would conquer another. We can find these convergences by searching for commas in the domain column. To detect a pattern, we use the str_detect function. The fixed function tells str_detect that we are looking for a fixed string (a comma) rather than a regular expression. str_detect returns a logical vector that we can use for an index:

```{r}
library(stringr)
english_monarchs$domain
multiple_kingdoms <- str_detect(english_monarchs$domain, fixed(","))
english_monarchs[multiple_kingdoms, c("name", "domain")]
```

